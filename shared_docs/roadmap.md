# 🎯 Multimodal AI 1-Year Roadmap

**1년간의 학습 & 연구 로드맵**을 월별 체크리스트로 정리한 것

---

## ✅ 1–3개월 (기초 지식 강화)
### Computer Vision (CV)
- [ ] OpenCV / Pillow 기초 (이미지 필터, 변환, 채널 분리)
- [ ] CNN, ResNet 구조 이해 및 구현
- [ ] YOLO Tiny 모델로 간단한 객체 탐지
- [ ] 미니 프로젝트: 이미지 분류기 / 얼굴 인식

### Natural Language Processing (NLP)
- [ ] 텍스트 전처리 (토크나이징, 임베딩)
- [ ] RNN, LSTM 기본 구현
- [ ] Transformer 기본 개념 학습
- [ ] 미니 프로젝트: 텍스트 분류, 감정 분석

### Programming & Frameworks
- [ ] Python / NumPy / Pandas 기초 정리
- [ ] PyTorch 튜토리얼 완료

---

## ✅ 4–6개월 (심화 이론 & 논문 구현)
### CV 심화
- [ ] GAN 구현 (MNIST/Face dataset)
- [ ] VAE 구현
- [ ] Style Transfer 프로젝트
- [ ] 논문 읽기: YOLOv4, Vision Transformer

### NLP 심화
- [ ] BERT fine-tuning (감정 분석 or QA)
- [ ] GPT-2/3 개념 및 코드 실습
- [ ] T5 논문 요약/텍스트 변환 실습
- [ ] 논문 읽기: BERT, GPT, T5

### Multimodal 시작
- [ ] CLIP 논문 재현 (이미지–텍스트 매칭)
- [ ] VisualBERT/Flamingo 논문 리뷰
- [ ] 미니 프로젝트: 텍스트 기반 이미지 검색

---

## ✅ 7–9개월 (중급 프로젝트 & 응용)
### Multimodal Projects
- [ ] VQA (Visual Question Answering) 모델 구현
- [ ] 이미지 캡셔닝 (Show & Tell / Transformer 기반)
- [ ] 추천 시스템 구현 (CF + Content-Based Filtering)
- [ ] 관심 도메인 프로젝트 기획 (자율주행, 의료영상 등)

### 논문 구현
- [ ] CLIP, DALL·E, ViLBERT 중 1~2개 재현
- [ ] Ablation study 추가 실험

---

## ✅ 10–12개월 (고급 프로젝트 & 연구 결과 정리)
### 고급 Multimodal AI
- [ ] 비디오 분석 (Action recognition, Video captioning)
- [ ] 오디오+텍스트 (Speech-to-Text or Audio Captioning)
- [ ] 감정 인식 멀티모달 시스템 구축
- [ ] 자율주행 객체 추적 & 경로 예측

### 논문 기반 연구
- [ ] Multimodal NMT 구현
- [ ] Video Captioning with Attention 논문 재현
- [ ] 논문 기반 개선 아이디어 제안 및 실험

---

## 📚 상시 계획
- [ ] 매일: 1시간 이상 코딩/논문 읽기
- [ ] 매주: 최소 1개 미니 프로젝트/실습 정리 → GitHub 업로드
- [ ] 매달: 논문 1~2편 읽고 요약 (→ `shared_docs/papers_summary.md`)
- [ ] Kaggle/AI 대회 참가, 커뮤니티 활동 기록
